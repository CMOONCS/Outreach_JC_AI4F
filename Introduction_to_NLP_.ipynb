{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPutQR1+dVVh1PtuDczm4u4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CMOONCS/Outreach_JC_AI4F/blob/main/Introduction_to_NLP_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A. NLP Text Preprocessing"
      ],
      "metadata": {
        "id": "S16QUH9E1iJs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1gZbyLoxhhq"
      },
      "outputs": [],
      "source": [
        "#@title NLP Tokenization\n",
        "\n",
        "text = \" \"\n",
        "\n",
        "print (\"Word-level Tokenization\")\n",
        "print(text.split())\n",
        "print(f\"# tokens: {len(text.split())}\")\n",
        "print (\"Sentence-Level Tokenization\")\n",
        "print(text.split(.))\n",
        "print(f\"# tokens: {len(text.split())}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lets try another Tokenizer\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "print (\"Word-level Tokenization\")\n",
        "print(word_tokenize(text))\n",
        "print(f\"# tokens: {len(word_tokenize(text))}\")\n",
        "print (\"Sentence-level Tokenization\")\n",
        "print(sent_tokenize(text))\n",
        "print(f\"# tokens: {len(sent_tokenize(text))}\")\n"
      ],
      "metadata": {
        "id": "3G_Ew1It3YOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Stemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "p_stemmer = PorterStemmer()\n",
        "words = ['run','runner','ran','runs','easily','fairly','fairness','generous','generation','gene']\n",
        "for word in words:\n",
        "    print(word + '---->' + p_stemmer.stem(word))\n",
        ""
      ],
      "metadata": {
        "id": "zWa49vqx4hRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lemmatization\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"I am a runner running in a race because I love to run since I ran today\")\n",
        "def show_lemmas(text):\n",
        "    for token in text:\n",
        "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')\n",
        "show_lemmas(doc)"
      ],
      "metadata": {
        "id": "gRg3NZ2d59PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Stopwords\n",
        "print(nlp.Defaults.stop_words)"
      ],
      "metadata": {
        "id": "PW28U1rN6g5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Checking whether words is stopword?\n",
        "nlp.vocab['is'].is_stop"
      ],
      "metadata": {
        "id": "Bj68PRU06ySF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Adding our own rules stopwords.\n",
        "nlp.Defaults.stop_words.add('btw')"
      ],
      "metadata": {
        "id": "8xO1lB4K64GW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}